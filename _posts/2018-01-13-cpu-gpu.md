---
layout: post
title:  cpu、gpu
date:   2018-01-13 14:30:00
category: "并发"
keywords: gpu 挖矿 mapReduce 众核 并行
---

# CPU、GPU 区别  
## 概念
首先需要解释 CPU 和 GPU 这两个缩写分别代表什么。CPU 即中央处理器，GPU 即图形处理器。其次，要解释两者的区别，要先明白两者的相同之处：两者都有总线和外界联系，有自己的缓存体系，以及数字和逻辑运算单元。一句话，两者都为了完成计算任务而设计。  

两者的区别在于存在于片内的缓存体系和数字逻辑运算单元的结构差异：CPU 虽然有多核，但总数没有超过两位数，每个核都有足够大的缓存和足够多的数字和逻辑运算单元，并辅助有很多加速分支判断甚至更复杂的逻辑判断的硬件；GPU 的核数远超 CPU，被称为众核（NVIDIA Fermi有512个核）。每个核拥有的缓存大小相对小，数字逻辑运算单元也少而简单（GPU初始时在浮点计算上一直弱于CPU）。从结果上导致 CPU 擅长处理具有复杂计算步骤和复杂数据依赖的计算任务，如分布式计算，数据压缩，人工智能，物理模拟，以及其他很多很多计算任务等。GPU 由于历史原因，是为了视频游戏而产生的（至今其主要驱动力还是不断增长的视频游戏市场），在三维游戏中常常出现的一类操作是对海量数据进行相同的操作，如：对每一个顶点进行同样的坐标变换，对每一个顶点按照同样的光照模型计算颜色值。GPU 的众核架构非常适合把同样的指令流并行发送到众核上，采用不同的输入数据执行。在 2003-2004 年左右，图形学之外的领域专家开始注意到GPU 与众不同的计算能力，开始尝试把 GPU 用于通用计算（即GPGPU）。之后 NVIDIA 发布了 CUDA，AMD 和 Apple 等公司也发布了 OpenCL，GPU 开始在通用计算领域得到广泛应用，包括：数值分析，海量数据处理（排序，Map-Reduce等），金融分析等等。  

简而言之，当程序员为 CPU 编写程序时，他们倾向于利用复杂的逻辑结构优化算法从而减少计算任务的运行时间，即 Latency。当程序员为 GPU 编写程序时，则利用其处理海量数据的优势，通过提高总的数据吞吐量（Throughput）来掩盖 Lantency。目前，CPU 和 GPU 的区别正在逐渐缩小，因为 GPU 也在处理不规则任务和线程间通信方面有了长足的进步。另外，功耗问题对于 GPU 比 CPU 更严重。

## 设计

CPU 和 GPU 之所以大不相同，是由于其设计目标的不同，它们分别针对了两种不同的应用场景。CPU 需要很强的通用性来处理各种不同的数据类型，同时又要逻辑判断又会引入大量的分支跳转和中断的处理。这些都使得 CPU 的内部结构异常复杂。而GPU面对的则是类型高度统一的、相互无依赖的大规模数据和不需要被打断的纯净的计算环境。

于是 CPU 和 GPU 就呈现出非常不同的架构（示意图）：

![cpu_gpu](/images/posts/cpu_gpu/cpu_gpu_arc.jpg)

图片来自 nVidia CUDA 文档。其中绿色的是计算单元，橙红色的是存储单元，橙黄色的是控制单元。

GPU 采用了数量众多的计算单元和超长的流水线，但只有非常简单的控制逻辑并省去了 Cache。而 CPU 不仅被 Cache 占据了大量空间，而且还有有复杂的控制逻辑和诸多优化电路，相比之下计算能力只是 CPU 很小的一部分

![cpu_gpu](/images/posts/cpu_gpu/oriention.jpg)

从上图可以看出：

Cache, local memory： CPU > GPU  
Threads(线程数): GPU > CPU  
Registers: GPU > CPU  多寄存器可以支持非常多的 Thread,thread 需要用到 register,thread数目大，register 也必须得跟着很大才行。  
SIMD Unit(单指令多数据流,以同步方式，在同一时间内执行同一条指令): GPU > CPU。  

CPU 基于低延时的设计：  

![cpu_gpu](/images/posts/cpu_gpu/latency_oriented.jpg)  

CPU 有强大的 ALU（算术运算单元）,它可以在很少的时钟周期内完成算术计算。  
当今的 CPU 可以达到 64bit 双精度。执行双精度浮点源算的加法和乘法只需要 1～3 个时钟周期。  
CPU 的时钟周期的频率是非常高的，达到 1.532～3 gigahertz(千兆HZ, 10的9次方).  
大的缓存也可以降低延时。保存很多的数据放在缓存里面，当需要访问的这些数据，只要在之前访问过的，如今直接在缓存里面取即可。
复杂的逻辑控制单元。当程序含有多个分支的时候，它通过提供分支预测的能力来降低延时。
数据转发。 当一些指令依赖前面的指令结果时，数据转发的逻辑控制单元决定这些指令在 pipeline 中的位置并且尽可能快的转发一个指令的结果给后续的指令。这些动作需要很多的对比电路单元和转发电路单元。 

![cpu_gpu](/images/posts/cpu_gpu/throughout_oriented.jpg)  

GPU 是基于大的吞吐量设计。

GPU 的特点是有很多的ALU和很少的 cache. 缓存的目的不是保存后面需要访问的数据的，这点和 CPU 不同，而是为 thread 提高服务的。如果有很多线程需要访问同一个相同的数据，缓存会合并这些访问，然后再去访问 dram（因为需要访问的数据保存在 dram 中而不是 cache 里面），获取数据后 cache 会转发这个数据给对应的线程，这个时候是数据转发的角色。但是由于需要访问 dram，自然会带来延时的问题。

GPU 的控制单元（左边黄色区域块）可以把多个的访问合并成少的访问。

GPU 的虽然有 dram 延时，却有非常多的 ALU 和非常多的 thread. 为啦平衡内存延时的问题，我们可以中充分利用多的 ALU 的特性达到一个非常大的吞吐量的效果。尽可能多的分配多的 Threads.通常来看GPU ALU 会有非常重的 pipeline 就是因为这样。

### 一个教授与一百个小学生 

所以与 CPU 擅长逻辑控制，串行的运算。和通用类型数据运算不同，GPU 擅长的是大规模并发计算，这也正是密码破解等所需要的。所以GPU除了图像处理，也越来越多的参与到计算当中来。GPU 的工作大部分就是这样，计算量大，但没什么技术含量，而且要重复很多很多次。就像你有个工作需要算几亿次一百以内加减乘除一样，最好的办法就是雇上几十个小学生一起算，一人算一部分，反正这些计算也没什么技术含量，纯粹体力活而已。而 CPU 就像老教授，积分微分都会算，就是工资高，一个老教授资顶二十个小学生，你要是富士康你雇哪个？GPU 就是这样，用很多简单的计算单元去完成大量的计算任务，纯粹的人海战术。这种策略基于一个前提，就是小学生 A 和小学生 B 的工作没有什么依赖性，是互相独立的。很多涉及到大量计算的问题基本都有这种特性，比如你说的破解密码，挖矿和很多图形学的计算。这些计算可以分解为多个相同的简单小任务，每个任务就可以分给一个小学生去做。但还有一些任务涉及到“流”的问题。比如你去相亲，双方看着顺眼才能继续发展。总不能你这边还没见面呢，那边找人把证都给领了。这种比较复杂的问题都是CPU来做的。  

总而言之，CPU 和 GPU 因为最初用来处理的任务就不同，所以设计上有不小的区别。而某些任务和 GPU 最初用来解决的问题比较相似，所以用 GPU 来算了。GPU 的运算速度取决于雇了多少小学生，CPU 的运算速度取决于请了多么厉害的教授。教授处理复杂任务的能力是碾压小学生的，但是对于没那么复杂的任务，还是顶不住人多。当然现在的GPU也能做一些稍微复杂的工作了，相当于升级成初中生高中生的水平。但还需要CPU 来把数据喂到嘴边才能开始干活，究竟还是靠 CPU 来管的。

什么类型的程序适合在 GPU 上运行？

（1）计算密集型的程序。所谓计算密集型(Compute-intensive)的程序，就是其大部分运行时间花在了寄存器运算上，寄存器的速度和处理器的速度相当，从寄存器读写数据几乎没有延时。可以做一下对比，读内存的延迟大概是几百个时钟周期；读硬盘的速度就不说了，即便是 SSD, 也实在是太慢了。　　（2）易于并行的程序。GPU 其实是一种 SIMD(Single Instruction Multiple Data)架构， 他有成百上千个核，每一个核在同一时间最好能做同样的事情。


## 参考  
[知乎](https://www.zhihu.com/question/19903344/answer/96081382)  
